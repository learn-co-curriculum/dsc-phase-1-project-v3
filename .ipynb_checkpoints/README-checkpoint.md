# Summary of Project
For this project we were tasked with identifying airplanes to purchase for a company that is new to the space by minimizing the risk that they pose to the company to buy. We were provided with a Kaggle dataset with information from the National Transportation Safety Board on flight incidents and accidents dating from 1962 to 2023. To identify which airplanes were the safest to purchase we decided on judging each airplane based on how well they kept passengers form sustaining major or lethal injuries in the event of an accident. We also investigated other factors to make recommendations including engine specifications, location, purpose of flight, weather, and the phase of flight that an accident occurs in. 
# Data Science Steps
The first data science step we took was importing the data, it was stored in a csv so we just used Pandas read function to turn it into a dataframe. After we had imported the data we began exploring it. To do this we got the info on the column names, their data types, and the number of null values contained in each column. We explored each column individually, finding the value counts of each column and refrencing the NTSB's data dictionary to find the meaning of abbreviations and terms. Finally we looked at the descriptive statistics of the numerical columns to try to find insights there. 
After the exploration phase we started cleaning our data. The first thing we did to clean our data was to drop columns that were not important to us either because they were metadata that wouldn't provide any insights, homogeneous data that only consisted of one value, or data that we did not deem useful for analysis. The next step of cleaning was getting rid of all planes that were amateur built, as it wouldn't be feasable to recommend amateur built planes as they are each unique. We then dropped that column as we had isolated it to only professionally built planes. After this we decided to deal with the null values present in our injury column by imputing a 0 for each cell with a null in it. We decided to do this as we assumed that the NTSB would be very dilligent about collecting injury information and wouldn't miss documenting any injuries that took place in an accident. 
After we had imputed this data we created our new columns that we would use for analysis. We created a variable for the number of passengers on the flight, one for how many people were injured on the flight, the percent of passengers that were seriously or fatally injured, and the percent of passengers that were uninjured. We then cleaned out all of the events where the aircraft was not an airplane, as well as the events before 1982 as there were only a few instances of cases before 1982 and we figured that we would not want to recommend an airplane that old anyways. We did a bit more cleaning by dropping events that had no passenger information as those would not be useful for our analysis. We then dropped all makes and models of planes with less than 30 events in the dataset as that is the commonly accepted number of data points needed to claim statistical significance. The final piece we added to the dataset was a column for the maximum capacity of each aircraft which is available online. 
After our dataset was ready we constructed smaller dataframes from aggregating the mean major or fatal injury rate for each of the variables that we were interested in looking at. We also included data on the maximum capacity, percent of uninjured passengers, and number of times that make and model showed up in the dataset in our aggregating dataframes. 
With this data we were able to sort it by best serious and fatal injury rate and find which makes and models, engine types, number of engines, locations, phases of flight, and purposes of flight had the best outcomes. 
Finally, we visualized these statistics using matplotlib and put everything together in a final jupyter notebook.
# Presentation and Sources
Presentation [link](https://docs.google.com/presentation/d/1_GSAe2idRKo77-N69BXIMNjis2SDb4ee92bYY-B1Dyo/edit#slide=id.p)
[kaggle dataset](https://www.kaggle.com/datasets/khsamaha/aviation-accident-database-synopses)
[NTSB data dictionary](https://www.ntsb.gov/Pages/AviationDownloadDataDictionary.aspx)
[Boeing 777 engine type](https://www.geaerospace.com/propulsion/commercial/ge9x)
[Refrence for the DC restricted airspace](https://www.faa.gov/newsroom/restricted-airspace-0)
[VMC complete definition](https://www.faa.gov/air_traffic/publications/atpubs/pcg_html/glossary-v.html#:~:text=VISUAL%20METEOROLOGICAL%20CONDITIONS%2D%20Meteorological%20conditions,(See%20INSTRUMENT%20FLIGHT%20RULES.))
[IMC complete definition](https://www.faa.gov/air_traffic/publications/atpubs/pcg_html/glossary-i.html#$INSTRUMENT%20METEOROLOGICAL%20CONDITIONS)
# Repository Navigation
Our repository contains two primary folders, one called data which contains all of our data csv files, and one called notebooks which contains all of our jupyter notebooks. When looking at the notebooks you should look at Data_Exploration_Cleaning first, run all cells in the notebook, then open the Visualizations_Colby and set the Kernel to run off of to be the Data_Exploration_Cleaning kernel and run all cells, and finally open the Final_Notebook and set the Kernel to run off of as Visualizations_Colby, and then run the entire notebook. 
# Tableau Dashboard
[Link](https://public.tableau.com/app/profile/colby.gates/viz/AviationRiskAnalysisProject1/AviationRiskAnalysis)